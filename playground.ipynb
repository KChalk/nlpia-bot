{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-b8224b8b86ba>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-b8224b8b86ba>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    return model\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nlp = load('en_core_web_md')\n",
    "TITLES = ['Chatbot', 'ELIZA', 'Turing_test', 'AIML', 'Chatterbot', 'Loebner_prize', 'Chinese_room']\n",
    "EXCLUDE_HEADINGS = ['See also', 'References', 'Bibliography', 'External links']\n",
    "\n",
    "\n",
    "class WikiIndex():\n",
    "    _url = 'https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-all-titles-in-ns0.gz'\n",
    "\n",
    "    def __init__(self, url=None, refresh=False, **pd_kwargs):\n",
    "        self._url = url or self._url\n",
    "        self.df_titles = self.load(url=self._url, refresh=refresh, **pd_kwargs)\n",
    "        self.title_row = dict(zip(self.df_titles.index.values, range(len(self.df_titles))))\n",
    "        self.df_vectors = self.compute_vectors()\n",
    "        \n",
    "    def compute_vectors(self, filename='wikipedia-title-vectors.csv.gz'):\n",
    "        filepath = os.path.join(constants.DATA_DIR, filename)\n",
    "        start = sum((1 for line in gzip.open(filepath, 'rb')))\n",
    "        total = len(self.df_titles) - start\n",
    "        vec_batch = []\n",
    "        with gzip.open(filepath, 'ta') as fout:\n",
    "            csv_writer = csv.writer(fout)\n",
    "            csv_writer.writerow(['page_title'] + [f'x{i}' for i in range(300)])\n",
    "            for i, s in tqdm(enumerate(self.df_titles.index.values[start:]), total=total):\n",
    "                vec = [s] + phrase_to_vec(str(s))  # s can sometimes (rarely) be a float because of pd.read_csv (df_titles)\n",
    "                vec_batch.append(vec)\n",
    "                if not (i % 1000) or i == total - 1:\n",
    "                    csv_writer.writerows(vec_batch)\n",
    "                    print(f\"wrote {len(vec_batch)} rows\")\n",
    "                    try:\n",
    "                        print(f'wrote {len(vec_batch), len(vec_batch[0])} values')\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                    vec_batch = []\n",
    "        time.sleep(1)\n",
    "        dtypes = {f'x{i}': pd.np.float16 for i in range(300)}\n",
    "        dtypes.update(page_title=str)\n",
    "        self.df_vectors = pd.read_csv(filepath, dtype=dtypes)\n",
    "        return self.df_vectors\n",
    "\n",
    "    def load(\n",
    "            self,\n",
    "            url='https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-all-titles-in-ns0.gz',\n",
    "            refresh=False,\n",
    "            **pd_kwargs):\n",
    "        url_dir, filename = os.path.split(url)\n",
    "        filepath = os.path.join(constants.DATA_DIR, filename)\n",
    "        df = None\n",
    "        \n",
    "        if not len(df):\n",
    "            df = pd.read_table(url, dtype=str)  # , sep=None, delimiter=None, quoting=3, engine='python')\n",
    " \n",
    "        df.columns = ['page_title']\n",
    "        if df.index.name != 'natural_title':\n",
    "            df.index = list(df['page_title'].str.replace('_', ' ').str.strip())\n",
    "            df.index.name == 'natural_title'\n",
    "            df.to_csv(filepath, index=False, compression='gzip')\n",
    "        self.df_titles = df\n",
    "        return self.df_titles\n",
    "\n",
    "    def find_similar_titles(self, title=None, n=1):\n",
    "        \"\"\" Takes dot product of a doc vector with all wikipedia title doc vectors to find closest article titles \"\"\"\n",
    "        if isinstance(title, str):\n",
    "            vec = nlp(title).vector\n",
    "        else:\n",
    "            vec = title\n",
    "        vec /= pd.np.linalg.norm(vec) or 1.\n",
    "        dot_products = vec.dot(self.df_vectors.values.T)\n",
    "        if n == 1:\n",
    "            return self.df_titles.index.values[dot_products.argmax()]\n",
    "        sorted(dot_products, reverse=True)\n",
    "\n",
    "def find_titles_vector(query='What is a chatbot?'):\n",
    "    \"\"\" Search vector representations of wikipedia titles for articles relevant to a statement or questions\n",
    "    \"\"\"\n",
    "    vector = phrase_to_vec(query)\n",
    "    closest = closest_vecs(vector, )\n",
    "    titles = find_titles(query)\n",
    "    titles = sorted(((len(t), t) for t in titles), reverse=True)\n",
    "    log.info(titles)\n",
    "    titles = [t for (n, t) in titles]\n",
    "    log.info(titles)\n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrape_wikipedia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-106bde5ca7d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_wikipedia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scrape_wikipedia' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
